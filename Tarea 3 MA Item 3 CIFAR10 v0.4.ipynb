{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Reconocimiento de Imágenes en CIFAR10\n",
    "\n",
    "En esta sección se trabajara con un dataset muy utilizado para experimentar con reconocimiento de imágenes: CIFAR10. Se trata de un conjunto de 60.000 imágenes RGB de $32x32$ pixeles que contiene 10 clases de objetos (6000 ejemplos por clase). La versión utilizada viene separada en 50.000 ejemplos de entrenamiento y 10.000 casos de prueba. El conjunto de pruebas fue obtenido seleccionando 1.000 imágenes aleatorias de cada clase. Los datos restantes han sido ordenados aleatoriamente y están organizados en 5 bloques de entrenamiento (batches). Las clases son mutuamente excluyentes y corresponden a las siguientes\n",
    "categorías: gato, perro, rana, caballo, pájaro, ciervo, avión, automóvil, camión y barco.\n",
    "\n",
    "\n",
    "### 2.3.a. Construcción de funcion para cargar datos de entrenamiento, prueba y validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Carga de diccionarios\n",
    "def unpickle(file):\n",
    "    import cPickle\n",
    "    fo = open(file, 'rb')\n",
    "    dict = cPickle.load(fo)\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "label_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog','frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1.a\n",
    "from scipy.misc import imread\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def load_CIFAR_one(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = pickle.load(f)\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        return X, np.array(Y, dtype=int)\n",
    "\n",
    "def load_CIFAR10(PATH):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1,6):\n",
    "        f = os.path.join(PATH, 'data_batch_%d' % (b, ))\n",
    "        X, Y = load_CIFAR_one(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtemp = np.concatenate(xs)\n",
    "    Ytemp = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xte, Yte = load_CIFAR_one(os.path.join(PATH, 'test_batch'))\n",
    "    Xtr, Xv, Ytr, Yv = train_test_split(Xtemp, Ytemp, test_size=0.2, random_state=0)\n",
    "    del Xtemp, Ytemp\n",
    "    return Xtr, Ytr, Xte, Yte, Xv, Yv\n",
    "\n",
    "Xtr, Ytr, Xte, Yte, Xv, Yv = load_CIFAR10('.') #you need to add Xval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la función load_CIFAR10() se genera las matrices de datos de entrenamiento $X_{tr}$ y $Y_{tr}$ de tamaño $50.000x32x32$, matrices de datos de prueba $X_{t}$ y $Y_{t}$ de tamaño $10.000x32x32$ y matrices de datos de validación $X_{v}$ y $Y_{v}$ de tamaño $10.000x32x32$.\n",
    "\n",
    "\n",
    "### 2.3.b. Construcción de funcion para escalar apropiadamente las imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bampo/.local/lib/python2.7/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#3.b\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "def scaler_function(Xtr,Xt,Xv,scale=True):\n",
    "    scaler = StandardScaler(with_std=scale).fit(Xtr)\n",
    "    Xtr_scaled = scaler.transform(Xtr)\n",
    "    Xt_scaled = scaler.transform(Xt)\n",
    "    Xv_scaled = scaler.transform(Xv)\n",
    "    return Xtr_scaled, Xt_scaled, Xv_scaled\n",
    "\n",
    "Xtr, Xte, Xv = scaler_function(Xtr,Xte,Xv)\n",
    "\n",
    "Ytr = to_categorical(Ytr)\n",
    "Yte = to_categorical(Yte)\n",
    "Yv = to_categorical(Yv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define la funcion scaler_function() la cual escalara los datos de cada pixel en formato RGB.\n",
    "\n",
    "### 2.3.c. Red neuronal para clasificación del problema CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9952/10000 [============================>.] - ETA: 0s\n",
      "Accuracy de train: 0.819920\n",
      "Accuracy de validacion: 0.820320\n",
      "Accuracy de test: 0.820000\n"
     ]
    }
   ],
   "source": [
    "#3.c\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "def do_NN(Xtr, Ytr, Xte, Yte, Xv, Yv):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=Xtr.shape[1], init='uniform', activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(10, init='uniform', activation='softmax'))\n",
    "    model.compile(optimizer=SGD(lr=0.05), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(Xtr, Ytr, nb_epoch=50, batch_size=32, verbose=0, validation_data=(Xv,Yv))\n",
    "    \n",
    "    scores = model.evaluate(Xtr, Ytr)\n",
    "    train_acc = scores[1]\n",
    "    scores = model.evaluate(Xv, Yv)\n",
    "    val_acc = scores[1]\n",
    "    scores = model.evaluate(Xte, Yte)\n",
    "    test_acc = scores[1]\n",
    "    \n",
    "    print \"\\nAccuracy de train: %f\"%(train_acc)\n",
    "    print \"Accuracy de validacion: %f\"%(val_acc)\n",
    "    print \"Accuracy de test: %f\"%(test_acc)\n",
    "    \n",
    "do_NN(Xtr, Ytr, Xte, Yte, Xv, Yv) #Se entrena y evalua la red neuronal con 1 capa oculta de 100 neuronas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de entrenar una red neuronal con 1 capa oculta de 100 neuronas, se obtuvo buenos resultados, para los datos de prueba se obtuvo un rendimiento de $81.99\\%$, en los datos de prueba un $82\\%$ y en los datos de validación un $82.03\\%$.\n",
    "\n",
    "### 2.3.d. Red neuronal para clasificación del problema CIFAR10 utilizando representaciones de histograma de color y descriptores HOG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Se vuelven a cargar los datos para realizar el siguiente experimento.\n",
    "Xtr, Ytr, Xte, Yte, Xv, Yv = load_CIFAR10('.')\n",
    "Ytr = to_categorical(Ytr)\n",
    "Yte = to_categorical(Yte)\n",
    "Yv = to_categorical(Yv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n",
      "(40000, 3072)\n",
      "(40000, 10)\n"
     ]
    }
   ],
   "source": [
    "#1.d\n",
    "from top_level_features import color_histogram_hsv\n",
    "from top_level_features import hog_features\n",
    "from top_level_features import extract_features\n",
    "\n",
    "features_train = extract_features(Xtr,[color_histogram_hsv]) #extrae histogramas de color\n",
    "features_test = extract_features(Xte,[color_histogram_hsv]) #extrae histogramas de color\n",
    "features_val = extract_features(Xv,[color_histogram_hsv]) #extrae histogramas de color\n",
    "\n",
    "print Xtr.shape\n",
    "print features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 1s     \n",
      " 9888/10000 [============================>.] - ETA: 0s\n",
      "Accuracy de train: 0.900000\n",
      "Accuracy de validacion: 0.900000\n",
      "Accuracy de test: 0.900000\n"
     ]
    }
   ],
   "source": [
    "do_NN(features_train, Ytr, features_test, Yte, features_val, Yv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al utilizar los datos de histogramas de color para las matrices $X_{tr}$, $X_{t}$ y $X_{v}$, se obtiene que el rendimiento al entrenar una red neuronal con 1 capa oculta de 100 neuronas es $90\\%$ para los 3 conjuntos de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n",
      "(40000, 3072)\n",
      "(40000, 144)\n"
     ]
    }
   ],
   "source": [
    "features_train = extract_features(Xtr,[hog_features]) #extrae hog features\n",
    "features_test = extract_features(Xte,[hog_features]) #extrae hog features\n",
    "features_val = extract_features(Xv,[hog_features]) #extrae hog features\n",
    "\n",
    "print Xtr.shape\n",
    "print features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9824/10000 [============================>.] - ETA: 0s\n",
      "Accuracy de train: 0.939097\n",
      "Accuracy de validacion: 0.923510\n",
      "Accuracy de test: 0.923050\n"
     ]
    }
   ],
   "source": [
    "do_NN(features_train, Ytr, features_test, Yte, features_val, Yv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por otro lado, al utilizar los datos de HOG para las matrices $X_{tr}$, $X_{t}$ y $X_{v}$, se obtiene que el rendimiento al entrenar una red neuronal con 1 capa oculta de 100 neuronas es $93.9\\%$ para los datos de entrenamiento, $92.3\\%$ para los datos de prueba y $92.35\\%$ para los datos de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n",
      "(40000, 3072)\n",
      "(40000, 154)\n"
     ]
    }
   ],
   "source": [
    "features_train = extract_features(Xtr,[hog_features, color_histogram_hsv]) #extrae todo\n",
    "features_test = extract_features(Xte,[hog_features, color_histogram_hsv]) #extrae todo\n",
    "features_val = extract_features(Xv,[hog_features, color_histogram_hsv]) #extrae todo\n",
    "\n",
    "print Xtr.shape\n",
    "print features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9536/10000 [===========================>..] - ETA: 0s\n",
      "Accuracy de train: 0.941872\n",
      "Accuracy de validacion: 0.926120\n",
      "Accuracy de test: 0.924690\n"
     ]
    }
   ],
   "source": [
    "do_NN(features_train, Ytr, features_test, Yte, features_val, Yv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, al utilizar los datos de histogramas de color y los datos de HOG en conjunto para las matrices $X_{tr}$, $X_{t}$ y $X_{v}$, se obtiene que el rendimiento al entrenar una red neuronal con 1 capa oculta de 100 neuronas es $94.18\\%$ para los datos de entrenamiento, $92.46\\%$ para los datos de prueba y $92.61\\%$ para los datos de validación.\n",
    "\n",
    "Como se puede notar, en los 3 experimentos con distintos datos extraídos, el rendimiento fue bueno, donde el mínimo rendimiento fue $90\\%$ y el modelo que obtuvo los mejores resultados para los datos de entrenamiento, de prueba y de validación fue utilizando todas las representaciones simultáneamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.3.e. SVM no lineal para clasificación del problema CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Se vuelven a cargar los datos para el experimento con SVM, se utilizara solo 10.000 datos de entrenamiento.\n",
    "Xtr, Ytr, Xte, Yte, Xv, Yv = load_CIFAR10('.')\n",
    "Xtr = Xtr[0:10000]\n",
    "Ytr = Ytr[0:10000]\n",
    "Xtr, Xte, Xv = scaler_function(Xtr,Xte,Xv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def do_SVM_no_lineal(x,y,xt,yt,xv,yv,model_type,best):\n",
    "    Cs = np.logspace(-2, 4,base=2, num=7)\n",
    "    acc_val = []\n",
    "    models = []\n",
    "    for C in Cs:\n",
    "        if model_type == 'rbf':\n",
    "            clf = SVC(C=C, kernel='rbf')\n",
    "        elif model_type == 'poly':\n",
    "            clf = SVC(C=C, kernel='poly',degree=2, coef0=1)\n",
    "\n",
    "        clf=clf.fit(x,y)\n",
    "                \n",
    "        acc_val.append(clf.score(xv,yv))\n",
    "        models.append(clf)\n",
    "\n",
    "    if best:\n",
    "        best_ = acc_val.index(max(acc_val))\n",
    "        C_b = Cs[best_]\n",
    "        model_b = models[best_]\n",
    "        print \"Mejor parámetro de regularización C: %s\"%(C_b)        \n",
    "        print \"\\nAccuracy de entrenamiento: %f\"%(model_b.score(x,y))\n",
    "        print \"Accuracy de prueba: %f\"%(model_b.score(xt,yt))\n",
    "        print \"Mejor Accuracy de validación: %f\"%(max(acc_val))\n",
    "        \n",
    "        plt.figure(figsize=(10,5))\n",
    "        ax = plt.gca()\n",
    "        ax.plot(Cs,acc_val,label = 'Accuracy validación')\n",
    "        plt.xlabel('C', fontsize = 16)\n",
    "        plt.ylabel('Accuracy', fontsize = 16)\n",
    "        plt.title('SVM no Lineal con kernel %s'%(model_type), fontsize = 16)\n",
    "        ax.set_xscale('linear')\n",
    "        plt.show()\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_SVM_no_lineal(Xtr, Ytr, Xte, Yte, Xv, Yv, 'rbf', best=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imagen2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado los resultados de entrenar una SVM no lineal con kernel rbf, el mejor parametro C fue 4, con esto es obtuvo un accuracy de entrenamiento de $95.72\\%$, un accuracy de prueba de $48.71\\%$ y un accuracy de validación de $48.97\\%$. Como puede notarse solo los datos de entrenamiento tuvieron buen rendimiento, esto es un claro caso de overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_SVM_no_lineal(Xtr, Ytr, Xte, Yte, Xv, Yv, 'poly', best=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train = extract_features(Xtr,[color_histogram_hsv]) #extrae histogramas de color\n",
    "features_test = extract_features(Xte,[color_histogram_hsv]) #extrae histogramas de color\n",
    "features_val = extract_features(Xv,[color_histogram_hsv]) #extrae histogramas de color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_SVM_no_lineal(features_train, Ytr, features_test, features_val, Xv, Yv, 'rbf', best=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_SVM_no_lineal(features_train, Ytr, features_test, features_val, Xv, Yv, 'poly', best=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train = extract_features(Xtr,[hog_features]) #extrae hog features\n",
    "features_test = extract_features(Xte,[hog_features]) #extrae hog features\n",
    "features_val = extract_features(Xv,[hog_features]) #extrae hog features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_SVM_no_lineal(features_train, Ytr, features_test, features_val, Xv, Yv, 'rbf', best=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_SVM_no_lineal(features_train, Ytr, features_test, features_val, Xv, Yv, 'poly', best=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train = extract_features(Xtr,[hog_features, color_histogram_hsv]) #extrae todo\n",
    "features_test = extract_features(Xte,[hog_features, color_histogram_hsv]) #extrae todo\n",
    "features_val = extract_features(Xv,[hog_features, color_histogram_hsv]) #extrae todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_SVM_no_lineal(features_train, Ytr, features_test, features_val, Xv, Yv, 'rbf', best=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_SVM_no_lineal(features_train, Ytr, features_test, features_val, Xv, Yv, 'poly', best=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que la ejecución de esta sección requiere mucho tiempo de computo, no se alcanzó a tener más resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.f. Árbol de clasificación de múltiples niveles para el problema CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtr, Ytr, Xte, Yte, Xv, Yv = load_CIFAR10('.')\n",
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "\n",
    "def do_Tree(x,y,xt,yt,xv,yv):\n",
    "    N_ts = np.linspace(2, 20, num=20)\n",
    "    acc_val = []\n",
    "    models = []\n",
    "    for n_t in N_ts:\n",
    "        clf=Tree(criterion='gini',splitter='best',random_state=0,max_depth=n_t)\n",
    "        clf=clf.fit(x,y)\n",
    "        acc_val.append(clf.score(xv,yv))\n",
    "        models.append(clf)\n",
    "    best_ = acc_val.index(max(acc_val))            \n",
    "    model_best = models[best_]\n",
    "    \n",
    "    print \"Profundidad del mejor árbol = %d\"%model_best.tree_.max_depth\n",
    "    print \"Mejor Accuracy de validación: %f\"%(max(acc_val))\n",
    "    print \"\\nAccuracy de entrenamiento: %f\"%(model_best.score(x,y))\n",
    "    print \"Accuracy de prueba: %f\"%(model_best.score(xt,yt))\n",
    "    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    ax = plt.gca()\n",
    "    ax.plot(N_ts,acc_val,label = 'Accuracy validación')\n",
    "    plt.xlabel('N_t', fontsize = 16)\n",
    "    plt.ylabel('Accuracy', fontsize = 16)\n",
    "    plt.title('Árbol de clasificacion con múltiples niveles', fontsize = 16)\n",
    "    ax.set_xscale('linear')\n",
    "    plt.show()\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_Tree(Xtr, Ytr, Xte, Yte, Xv, Yv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imagen3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado los resultados de entrenar un arbol de clasificación de multiples niveles desde 2 hasta 20 niveles. El mejor arbol fue el de profundidad 9, con esto es obtuvo un accuracy de entrenamiento de $39.0\\%$, un accuracy de prueba de $30.2\\%$ y un accuracy de validación de $29.83\\%$. Como puede notarse el rendimiento de clasificar el problema de CIFAR10 mediante arboles no genera buenos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train = extract_features(Xtr,[color_histogram_hsv]) #extrae histogramas de color\n",
    "features_test = extract_features(Xte,[color_histogram_hsv]) #extrae histogramas de color\n",
    "features_val = extract_features(Xv,[color_histogram_hsv]) #extrae histogramas de color\n",
    "\n",
    "do_Tree(features_train, Ytr, features_test, Yte, features_val, Yv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train = extract_features(Xtr,[hog_features]) #extrae hog features\n",
    "features_test = extract_features(Xte,[hog_features]) #extrae hog features\n",
    "features_val = extract_features(Xv,[hog_features]) #extrae hog features\n",
    "\n",
    "do_Tree(features_train, Ytr, features_test, Yte, features_val, Yv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train = extract_features(Xtr,[hog_features, color_histogram_hsv]) #extrae todo\n",
    "features_test = extract_features(Xte,[hog_features, color_histogram_hsv]) #extrae todo\n",
    "features_val = extract_features(Xv,[hog_features, color_histogram_hsv]) #extrae todo\n",
    "\n",
    "do_Tree(features_train, Ytr, features_test, Yte, features_val, Yv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que la ejecución de esta sección requiere mucho tiempo de computo, no se alcanzó a tener más resultados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
